import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, StandardScaler
import tensorflow as tf
from tensorflow.keras import layers, models, callbacks
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix
import pickle
import os

# ==================== GPU CONFIGURATION ====================
print("="*70)
print("ğŸ” KIá»‚M TRA GPU CUDA")
print("="*70)

# Kiá»ƒm tra TensorFlow version
print(f"TensorFlow version: {tf.__version__}")

# Kiá»ƒm tra GPU cÃ³ sáºµn khÃ´ng
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    print(f"âœ… TÃ¬m tháº¥y {len(gpus)} GPU:")
    for i, gpu in enumerate(gpus):
        print(f"   GPU {i}: {gpu.name}")
        # Cáº¥u hÃ¬nh memory growth Ä‘á»ƒ khÃ´ng chiáº¿m háº¿t VRAM
        try:
            tf.config.experimental.set_memory_growth(gpu, True)
            print(f"   âœ… ÄÃ£ báº­t memory growth cho GPU {i}")
        except RuntimeError as e:
            print(f"   âš ï¸ KhÃ´ng thá»ƒ set memory growth: {e}")
    
    # Hiá»ƒn thá»‹ GPU hiá»‡n táº¡i Ä‘ang dÃ¹ng
    print(f"\nğŸš€ Training sáº½ sá»­ dá»¥ng: {gpus[0].name}")
else:
    print("âš ï¸ KHÃ”NG tÃ¬m tháº¥y GPU!")
    print("âš ï¸ Training sáº½ cháº¡y trÃªn CPU (cháº­m hÆ¡n)")
    print("\nğŸ’¡ Äá»ƒ sá»­ dá»¥ng GPU, cáº§n:")
    print("   1. CÃ i Ä‘áº·t CUDA Toolkit")
    print("   2. CÃ i Ä‘áº·t cuDNN")
    print("   3. CÃ i Ä‘áº·t tensorflow-gpu hoáº·c tensorflow>=2.0")

# Kiá»ƒm tra CUDA cÃ³ Ä‘Æ°á»£c build khÃ´ng
print(f"\nCUDA Available: {tf.test.is_built_with_cuda()}")
print(f"GPU Available: {tf.test.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None)}")

print("="*70 + "\n")
# ============================================================

# Äá»c dá»¯ liá»‡u tá»« extract_landmarks.py (63 features + label)
print("ğŸ“‚ Äá»c dá»¯ liá»‡u landmarks...")
train = pd.read_csv("landmarks_train.csv")
val = pd.read_csv("landmarks_val.csv")
test = pd.read_csv("landmarks_test.csv")

X_train, y_train = train.drop("label", axis=1).values, train["label"]
X_val, y_val = val.drop("label", axis=1).values, val["label"]
X_test, y_test = test.drop("label", axis=1).values, test["label"]

print(f"âœ… Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}")

# Chuáº©n hÃ³a dá»¯ liá»‡u (quan trá»ng vÃ¬ z coordinate chÆ°a Ä‘Æ°á»£c chuáº©n hÃ³a)
print("\nğŸ”§ Chuáº©n hÃ³a dá»¯ liá»‡u...")
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_val = scaler.transform(X_val)
X_test = scaler.transform(X_test)

# Encode labels
le = LabelEncoder()
y_train = le.fit_transform(y_train)
y_val = le.transform(y_val)
y_test = le.transform(y_test)

le_classes_fixed = []
for c in le.classes_:
    if c == "dd":
        le_classes_fixed.append("Ä‘")
    else:
        le_classes_fixed.append(c)
le.classes_ = np.array(le_classes_fixed)

num_classes = len(le.classes_)
print(f"âœ… Sá»‘ lá»›p: {num_classes}")
print(f"âœ… Classes: {list(le.classes_)}")

# XÃ¢y dá»±ng mÃ´ hÃ¬nh (phÃ¹ há»£p vá»›i 63 landmarks features)
print("\nğŸ—ï¸ XÃ¢y dá»±ng mÃ´ hÃ¬nh...")
model = models.Sequential([
    layers.Input((63,)),  # 21 landmarks Ã— 3 tá»a Ä‘á»™ (x,y,z)
    
    layers.Dense(256, activation="relu"),
    layers.BatchNormalization(),
    layers.Dropout(0.4),
    
    layers.Dense(128, activation="relu"),
    layers.BatchNormalization(),
    layers.Dropout(0.3),
    
    layers.Dense(64, activation="relu"),
    layers.Dropout(0.2),
    
    layers.Dense(num_classes, activation="softmax")
])

model.compile(
    optimizer="adam",
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

model.summary()

# Callbacks Ä‘á»ƒ tá»‘i Æ°u training
print("\nâš™ï¸ Thiáº¿t láº­p callbacks...")
cb = [
    callbacks.EarlyStopping(
        monitor='val_loss',
        patience=15,
        restore_best_weights=True,
        verbose=1
    ),
    callbacks.ModelCheckpoint(
        'best_vsl_landmarks_model.h5',
        monitor='val_accuracy',
        save_best_only=True,
        verbose=1
    ),
    callbacks.ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=5,
        min_lr=1e-6,
        verbose=1
    )
]

# Training
print("\nğŸš€ Báº¯t Ä‘áº§u training...")
history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=50,
    batch_size=32,
    callbacks=cb,
    verbose=1
)

# ÄÃ¡nh giÃ¡ trÃªn test set
print("\nğŸ“Š ÄÃ¡nh giÃ¡ trÃªn test set...")
test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)
print(f"âœ… Test Accuracy: {test_acc*100:.2f}%")
print(f"âœ… Test Loss: {test_loss:.4f}")

# Classification Report
y_pred = model.predict(X_test, verbose=0)
y_pred_classes = np.argmax(y_pred, axis=1)

print("\nğŸ“‹ Classification Report:")
print(classification_report(y_test, y_pred_classes, 
                          target_names=le.classes_,
                          digits=4))

# Confusion Matrix
print("\nğŸ“Š Táº¡o Confusion Matrix...")
cm = confusion_matrix(y_test, y_pred_classes)
plt.figure(figsize=(max(10, num_classes), max(8, num_classes-2)))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=le.classes_,
            yticklabels=le.classes_,
            cbar_kws={'label': 'Count'})
plt.title('Confusion Matrix - VSL Hand Landmarks Classification', fontsize=14, pad=20)
plt.ylabel('True Label', fontsize=12)
plt.xlabel('Predicted Label', fontsize=12)
plt.tight_layout()
plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')
print("âœ… Saved: confusion_matrix.png")
plt.close()

# Plot training history
print("\nğŸ“ˆ Táº¡o Training History Plot...")
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Accuracy
axes[0].plot(history.history['accuracy'], label='Train', linewidth=2)
axes[0].plot(history.history['val_accuracy'], label='Validation', linewidth=2)
axes[0].set_title('Model Accuracy', fontsize=14, fontweight='bold')
axes[0].set_xlabel('Epoch', fontsize=12)
axes[0].set_ylabel('Accuracy', fontsize=12)
axes[0].legend(fontsize=11)
axes[0].grid(True, alpha=0.3)

# Loss
axes[1].plot(history.history['loss'], label='Train', linewidth=2)
axes[1].plot(history.history['val_loss'], label='Validation', linewidth=2)
axes[1].set_title('Model Loss', fontsize=14, fontweight='bold')
axes[1].set_xlabel('Epoch', fontsize=12)
axes[1].set_ylabel('Loss', fontsize=12)
axes[1].legend(fontsize=11)
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('training_history.png', dpi=300, bbox_inches='tight')
print("âœ… Saved: training_history.png")
plt.close()

# LÆ°u model vÃ  preprocessors
print("\nğŸ’¾ LÆ°u model vÃ  preprocessors...")
model.save("vsl_landmarks_model.h5")
print("âœ… Saved: vsl_landmarks_model.h5")

# LÆ°u LabelEncoder vÃ  Scaler (cáº§n cho inference)
with open('label_encoder.pkl', 'wb') as f:
    pickle.dump(le, f)
with open('scaler.pkl', 'wb') as f:
    pickle.dump(scaler, f)
print("âœ… Saved: label_encoder.pkl")
print("âœ… Saved: scaler.pkl")

# Tá»•ng káº¿t
print("\n" + "="*60)
print("ğŸ‰ TRAINING HOÃ€N Táº¤T!")
print("="*60)
print(f"ğŸ“Š Final Test Accuracy: {test_acc*100:.2f}%")
print(f"ğŸ“Š Total Epochs Trained: {len(history.history['loss'])}")
print(f"ğŸ“Š Number of Classes: {num_classes}")
print(f"ğŸ“Š Classes: {list(le.classes_)}")
print("="*60)
